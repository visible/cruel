---
title: Getting Started
description: Chaos testing with zero mercy
---

Cruel is a chaos engineering library for testing how your AI applications handle failures. It wraps AI SDK models with configurable fault injection - rate limits, timeouts, stream cuts, corrupt responses, and more.

For the base `cruel(...)` function wrappers (fetch/services/Core API), see the [Core API](/docs/core) page.

## Install

<CodeBlockTabs defaultValue="bun">
  <CodeBlockTabsList>
    <CodeBlockTabsTrigger value="bun">bun</CodeBlockTabsTrigger>
    <CodeBlockTabsTrigger value="npm">npm</CodeBlockTabsTrigger>
    <CodeBlockTabsTrigger value="pnpm">pnpm</CodeBlockTabsTrigger>
  </CodeBlockTabsList>

  <CodeBlockTab value="bun">

```bash
bun add cruel
```

  </CodeBlockTab>

  <CodeBlockTab value="npm">

```bash
npm install cruel
```

  </CodeBlockTab>

  <CodeBlockTab value="pnpm">

```bash
pnpm add cruel
```

  </CodeBlockTab>
</CodeBlockTabs>

## Wrap a Model

```ts
import { openai } from "@ai-sdk/openai"
import { generateText } from "ai"
import { cruelModel } from "cruel/ai-sdk"

const model = cruelModel(openai("gpt-4o"), {
  rateLimit: 0.2,
  delay: [100, 500],
  onChaos: (event) => console.log(event.type, event.modelId),
})

const result = await generateText({
  model,
  prompt: "hello",
})
```

This wraps the model so 20% of calls get a 429 rate limit error and every call has 100-500ms of added latency. The `onChaos` callback fires whenever chaos is injected.

## How It Works

Cruel sits between your app and the AI provider. When a call is made:

1. **Pre-call checks** - rate limit, overloaded, timeout, etc. fire before the API call
2. **API call** - if pre-call passes, the real API call happens
3. **Post-call mutations** - partial response, token usage override, etc. modify the result
4. **Stream transforms** - slow tokens, corrupt chunks, stream cuts modify the stream

Errors use the same format as real provider errors (`APICallError.isInstance()` returns true), so your retry logic works exactly like it would in production.

## Streaming

```ts
import { streamText } from "ai"
import { cruelModel } from "cruel/ai-sdk"

const model = cruelModel(openai("gpt-4o"), {
  slowTokens: [50, 200],
  streamCut: 0.1,
  corruptChunks: 0.02,
})

const result = streamText({ model, prompt: "hello" })

for await (const chunk of result.fullStream) {
  if (chunk.type === "text-delta") process.stdout.write(chunk.delta)
}
```

## With the Gateway

```ts
import { gateway } from "@ai-sdk/gateway"
import { cruelModel } from "cruel/ai-sdk"

const model = cruelModel(gateway("openai/gpt-4o"), {
  rateLimit: 0.2,
  overloaded: 0.1,
})
```

## Presets

```ts
import { cruelModel, presets } from "cruel/ai-sdk"

cruelModel(openai("gpt-4o"), presets.realistic)
cruelModel(openai("gpt-4o"), presets.nightmare)
cruelModel(openai("gpt-4o"), presets.apocalypse)
```

## Wrap Everything

```ts
import { cruelProvider } from "cruel/ai-sdk"

const chaos = cruelProvider(openai, { rateLimit: 0.1 })

chaos("gpt-4o")                              // language model
chaos.embeddingModel("text-embedding-3-small") // embedding model
chaos.imageModel("dall-e-3")                   // image model
```

## Run the Examples

```bash
cd packages/examples
bun run run.ts ai-sdk openai
bun run run.ts ai-gateway anthropic
bun run run.ts core
bun run run.ts with-diagnostics
bun run run.ts ai-sdk openai -m gpt-6
bun run run.ts ai-gateway openai --model gpt-6
```

`-m` / `--model` sets `MODEL` for each matched example process. This swaps the model ID without changing example files.
